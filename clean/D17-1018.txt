End-to-end	O
Neural	O
Coreference	O
Resolution	O

We	O
present	O
the	O
first	O
state-of-the-art	O
coreference	O
resolution	O
model	B-NEU
that	O
is	O
learned	O
end-to-end	O
given	O
only	O
gold	O
mention	O
clusters	O
.	O

All	O
recent	O
coreference	O
models	O
,	O
including	O
neural	B-POS
approaches	I-POS
that	O
achieved	O
impressive	O
performance	O
gains	O
(	O
Wiseman	O
et	O
al.	O
,	O
2016	O
;	O
Clark	O
and	O
Manning	O
,	O
2016b,a	O
)	O
,	O
rely	O
on	O
syntactic	O
parsers	O
,	O
both	O
for	O
head-word	O
features	O
and	O
as	O
the	O
input	O
to	O
carefully	O
hand-engineered	O
mention	O
proposal	O
algorithms	O
.	O

We	O
demonstrate	O
for	O
the	O
first	O
time	O
that	O
these	O
resources	O
are	O
not	O
required	O
,	O
and	O
in	O
fact	O
performance	O
can	O
be	O
improved	O
significantly	O
without	O
them	O
,	O
by	O
training	O
an	B-POS
end-to-end	I-POS
neural	I-POS
model	I-POS
that	O
jointly	O
learns	O
which	O
spans	O
are	O
entity	O
mentions	O
and	O
how	O
to	O
best	O
cluster	O
them	O
.	O

Our	B-POS
model	I-POS
reasons	O
over	O
the	O
space	O
of	O
all	O
spans	O
up	O
to	O
a	O
maximum	O
length	O
and	O
directly	O
optimizes	O
the	O
marginal	O
likelihood	O
of	O
antecedent	O
spans	O
from	O
gold	O
coreference	O
clusters	O
.	O

It	O
includes	O
a	B-NEU
span-ranking	I-NEU
model	I-NEU
that	O
decides	O
,	O
for	O
each	O
span	O
,	O
which	O
of	O
the	O
previous	O
spans	O
(	O
if	O
any	O
)	O
is	O
a	O
good	O
antecedent	O
.	O

At	O
the	O
core	O
of	O
our	B-NEU
model	I-NEU
are	O
vector	O
embeddings	O
representing	O
spans	O
of	O
text	O
in	O
the	O
document	O
,	O
which	O
combine	O
context-dependent	O
boundary	O
representations	O
with	O
a	O
head-finding	O
attention	O
mechanism	O
over	O
the	O
span	O
.	O

The	B-NEU
attention	I-NEU
component	I-NEU
is	O
inspired	O
by	O
parser-derived	O
head-word	O
matching	O
features	O
from	O
previous	B-NEU
systems	I-NEU
(	O
Durrett	O
and	O
Klein O
, O
2013	O
)	O
,	O
but	O
is	O
less	O
susceptible	O
to	O
cascading	O
errors	O
.	O

In	O
our	O
analyses	O
,	O
we	O
show	O
empirically	O
that	O
these	B-POS
learned	I-POS
attention	I-POS
weights	I-POS
correlate	O
strongly	O
with	O
traditional	O
headedness	O
definitions	O
.	O

Scoring	O
all	O
span	O
pairs	O
in	O
our	O
end-to-end	O
model	O
is	O
impractical	O
,	O
since	O
the	O
complexity	O
would	O
be	O
quartic	O
in	O
the	O
document	O
length	O
.	O

Therefore	O
we	O
factor	O
the	B-NEU
model	I-NEU
over	O
unary	O
mention	O
scores	O
and	O
pairwise	O
antecedent	O
scores	O
,	O
both	O
of	O
which	O
are	O
simple	O
functions	O
of	O
the	O
learned	O
span	O
embedding	O
.	O

The	O
unary	O
mention	O
scores	O
are	O
used	O
to	O
prune	O
the	O
space	O
of	O
spans	O
and	O
antecedents	O
,	O
to	O
aggressively	O
reduce	O
the	O
number	O
of	O
pairwise	O
computations	O
.	O

Our	B-POS
final	I-POS
approach	I-POS
outperforms	O
existing	B-NEU
models	I-NEU
by	O
1.5	O
F1	O
on	O
the	O
OntoNotes	O
benchmark	O
and	O
by	O
3.1	O
F1	O
using	O
a	B-NEU
5-model	I-NEU
ensemble	I-NEU
. O

It	O
is	O
not	O
only	O
accurate	O
,	O
but	O
also	O
relatively	O
interpretable	O
.	O

The	O
model	O
factors	O
,	O
for	O
example	O
,	O
directly	O
indicate	O
whether	O
an	O
absent	O
coreference	O
link	O
is	O
due	O
to	O
low	O
mention	O
scores	O
(	O
for	O
either	O
span	O
)	O
or	O
a	O
low	O
score	O
from	O
the	O
mention	O
ranking	O
component	O
.	O

The	O
head-finding	O
attention	O
mechanism	O
also	O
reveals	O
which	O
mention-internal	O
words	O
contribute	O
most	O
to	O
coreference	O
decisions	O
.	O

We	O
leverage	O
this	O
overall	O
interpretability	O
to	O
do	O
detailed	O
quantitative	O
and	O
qualitative	O
analyses O
, O
providing	O
insights	O
into	O
the	O
strengths	O
and	O
weaknesses	O
of	O
the	B-NEU
approach	I-NEU
. O
