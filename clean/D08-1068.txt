Joint	O
Unsupervised	O
Coreference	O
Resolution	O
with	O
Markov	O
Logic	O

The	O
goal	O
of	O
coreference	O
resolution	O
is	O
to	O
identify	O
mentions	O
(	O
typically	O
noun	O
phrases	O
)	O
that	O
refer	O
to	O
the	O
same	O
entities	O
.	O

This	O
is	O
a	O
key	O
subtask	O
in	O
many	O
NLP	O
applications	O
,	O
including	O
information	O
extraction	O
,	O
question	O
answering	O
,	O
machine	O
translation	O
,	O
and	O
others	O
.	O

Supervised	B-NEU
learning	I-NEU
approaches	I-NEU
treat	O
the	O
problem	O
as	O
one	O
of	O
classification O
:	O
for	O
each	O
pair	O
of	O
mentions O
, O
predict	O
whether	O
they	O
corefer	O
or	O
not	O
(	O
e.g.	O
,	O
McCallum	O
&	O
Wellner	O
(	O
2005	O
)	O
)	O
.	O

While	O
successful	O
,	O
these	B-NEG
approaches	I-NEG
require	O
labeled	O
training	O
data	O
,	O
consisting	O
of	O
mention	O
pairs	O
and	O
the	O
correct	O
decisions	O
for	O
them	O
.	O

This	O
limits	O
their	O
applicability	O
.	O

Unsupervised	B-POS
approaches	I-POS
are	O
attractive	O
due	O
to	O
the	O
availability	O
of	O
large	O
quantities	O
of	O
unlabeled	O
text	O
.	O

However	O
,	O
unsupervised	O
coreference	O
resolution	O
is	O
much	O
more	O
difficult	O
.	O

Haghighi	B-NEG
and	I-NEG
Klein’s	I-NEG
(	I-NEG
2007 I-NEG
) I-NEG
model	I-NEG
the	O
most	O
sophisticated	O
to	O
date	O
,	O
still	O
lags	O
supervised	B-POS
ones	I-POS
by	O
a	O
substantial	O
margin	O
.	O

Extending	O
it	O
appears	O
difficult	O
,	O
due	O
to	O
the	O
limitations	O
of	O
its	O
Dirichlet	O
process-based	O
representation	O
.	O

The	O
lack	O
of	O
label	O
information	O
in	O
unsupervised	O
coreference	O
resolution	O
can	O
potentially	O
be	O
overcome	O
by	O
performing	O
joint	O
inference	O
,	O
which	O
leverages	O
the O
“ O
easy O
”	O
decisions	O
to	O
help	O
make	O
related	O
“ O
hard O
”	O
ones	O
.	O

Relations	O
that	O
have	O
been	O
exploited	O
in	O
supervised	O
coreference	O
resolution	O
include	O
transitivity	O
(	O
McCallum	O
&	O
Wellner	O
,	O
2005	O
)	O
and	O
anaphoricity	O
(	O
Denis	O
&Baldridge	O
,	O
2007	O
)	O
.	O

However	O
,	O
there	O
is	O
little	O
work	O
to	O
date	O
on	O
joint	O
inference	O
for	O
unsupervised	O
resolution	O
.	O

We	O
address	O
this	O
problem	O
using	O
Markov	B-POS
logic	I-POS
a	O
powerful	O
and	O
flexible	O
language	O
that	O
combines	O
probabilistic	B-NEU
graphical	I-NEU
models	I-NEU
and	O
first-order	O
logic O
( O
Richardson	O
&	O
Domingos	O
,	O
2006	O
)	O
.	O

Markov	O
logic	O
allows	O
us	O
to	O
easily	O
build	O
models	B-NEU
involving	O
relations	O
among	O
mentions	O
,	O
like	O
apposition	O
and	O
predicate	O
nominals	O
.	O

By	O
extending	O
the	B-NEU
state-of-the-art	I-NEU
algorithms	I-NEU
for	O
inference	O
and	O
learning	O
,	O
we	O
developed	O
the	O
first	O
general-purpose	B-POS
unsupervised	I-POS
learning	I-POS
algorithm	I-POS
for	O
Markov	O
logic	O
,	O
and	O
applied	O
it	O
to	O
unsupervised	O
coreference	O
resolution	O
.	O

We	O
test	O
our	B-NEU
approach	I-NEU
on	O
standard	O
MUC	O
and	O
ACE	O
datasets	O
.	O

Our	B-POS
basic	I-POS
model	I-POS
trained	O
on	O
a	O
minimum	O
of	O
data	O
,	O
suffices	O
to	O
outperform	O
Haghighi	B-NEU
and	I-NEU
Klein’s I-NEU
( I-NEU
2007	I-NEU
)	I-NEU
one	O
.	O

Our	B-POS
full	I-POS
model	I-POS
using	O
apposition	O
and	O
other	O
relations	O
for	O
joint	O
inference	O
,	O
is	O
often	O
as	O
accurate	O
as	O
the	B-NEU
best	I-NEU
supervised	I-NEU
models	I-NEU
or	O
more	O
.	O

We	O
begin	O
by	O
reviewing	O
the	O
necessary	O
background	O
on	O
Markov	O
logic	O
.	O

We	O
then	O
describe	O
our	O
Markov	O
logic	O
network	O
for	O
joint	O
unsupervised	O
coreference	O
resolution	O
,	O
and	O
the	O
learning	O
and	O
inference	O
algorithms	O
we	O
used	O
.	O

Finally	O
,	O
we	O
present	O
our	O
experiments	O
and	O
results	O
.	O
